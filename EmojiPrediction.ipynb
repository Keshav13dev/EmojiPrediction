{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7fJIuBWIwnVZyWyLPg6sB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Keshav13dev/Aptos_project/blob/main/EmojiPrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6uATL3FX3bID",
        "outputId": "c66ace11-ce73-4cc8-f46b-0e58eb5844ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label distribution:\n",
            " Label\n",
            "2    55\n",
            "3    43\n",
            "0    28\n",
            "4    22\n",
            "1    18\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_9\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_9\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_9 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │        \u001b[38;5;34m29,500\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_18 (\u001b[38;5;33mLSTM\u001b[0m)                  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_18 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_19 (\u001b[38;5;33mLSTM\u001b[0m)                  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_19 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">29,500</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m29,500\u001b[0m (115.23 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,500</span> (115.23 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m29,500\u001b[0m (115.23 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,500</span> (115.23 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 173ms/step - accuracy: 0.2708 - loss: 1.6051 - val_accuracy: 0.3529 - val_loss: 1.5162\n",
            "Epoch 2/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.3389 - loss: 1.5405 - val_accuracy: 0.3529 - val_loss: 1.4914\n",
            "Epoch 3/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.3493 - loss: 1.5026 - val_accuracy: 0.3529 - val_loss: 1.4744\n",
            "Epoch 4/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3806 - loss: 1.4574 - val_accuracy: 0.3529 - val_loss: 1.4615\n",
            "Epoch 5/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3804 - loss: 1.4386 - val_accuracy: 0.3235 - val_loss: 1.4535\n",
            "Epoch 6/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.4489 - loss: 1.3733 - val_accuracy: 0.2647 - val_loss: 1.4334\n",
            "Epoch 7/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.4744 - loss: 1.3382 - val_accuracy: 0.3235 - val_loss: 1.4182\n",
            "Epoch 8/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.5680 - loss: 1.2237 - val_accuracy: 0.2941 - val_loss: 1.4137\n",
            "Epoch 9/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.5503 - loss: 1.1842 - val_accuracy: 0.3529 - val_loss: 1.3966\n",
            "Epoch 10/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6547 - loss: 1.0362 - val_accuracy: 0.3824 - val_loss: 1.3703\n",
            "Epoch 11/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7097 - loss: 1.0031 - val_accuracy: 0.4118 - val_loss: 1.3623\n",
            "Epoch 12/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7309 - loss: 0.8923 - val_accuracy: 0.4118 - val_loss: 1.3680\n",
            "Epoch 13/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.7418 - loss: 0.8818 - val_accuracy: 0.4412 - val_loss: 1.3441\n",
            "Epoch 14/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7629 - loss: 0.8321 - val_accuracy: 0.4412 - val_loss: 1.3075\n",
            "Epoch 15/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7497 - loss: 0.7875 - val_accuracy: 0.5000 - val_loss: 1.3110\n",
            "Epoch 16/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8369 - loss: 0.6696 - val_accuracy: 0.6176 - val_loss: 1.1593\n",
            "Epoch 17/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.7947 - loss: 0.6736 - val_accuracy: 0.5000 - val_loss: 1.2550\n",
            "Epoch 18/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.8091 - loss: 0.5938 - val_accuracy: 0.5000 - val_loss: 1.4933\n",
            "Epoch 19/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.7749 - loss: 0.6866 - val_accuracy: 0.5882 - val_loss: 1.1860\n",
            "Epoch 20/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.7941 - loss: 0.5547 - val_accuracy: 0.5882 - val_loss: 1.1727\n",
            "Epoch 21/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8489 - loss: 0.4897 - val_accuracy: 0.4118 - val_loss: 1.3711\n",
            "Epoch 22/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8629 - loss: 0.5282 - val_accuracy: 0.5000 - val_loss: 1.1957\n",
            "Epoch 23/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8772 - loss: 0.4620 - val_accuracy: 0.5882 - val_loss: 1.1318\n",
            "Epoch 24/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8945 - loss: 0.4035 - val_accuracy: 0.5000 - val_loss: 1.3618\n",
            "Epoch 25/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8426 - loss: 0.4665 - val_accuracy: 0.5882 - val_loss: 1.2535\n",
            "Epoch 26/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8836 - loss: 0.3573 - val_accuracy: 0.5294 - val_loss: 1.3130\n",
            "Epoch 27/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8814 - loss: 0.4479 - val_accuracy: 0.5882 - val_loss: 1.2737\n",
            "Epoch 28/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9113 - loss: 0.3686 - val_accuracy: 0.5882 - val_loss: 1.2393\n",
            "Epoch 29/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9564 - loss: 0.2750 - val_accuracy: 0.5000 - val_loss: 1.4069\n",
            "Epoch 30/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9353 - loss: 0.3386 - val_accuracy: 0.6176 - val_loss: 1.4755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 367ms/step\n",
            "Input: i feel good | Predicted Emoji: 😃\n",
            "Input: i feel upset | Predicted Emoji: 😃\n",
            "Input: let's eat dinner | Predicted Emoji: 🍽️\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import emoji\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,LSTM, SimpleRNN, Embedding, Dropout\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "\n",
        "data = pd.read_csv('/content/sample_data/emoji_data.csv',header = None)\n",
        "data.columns = ['Text', 'Label']\n",
        "data = data.dropna()\n",
        "data = data[data['Label'].apply(lambda x: str(x).isdigit())]\n",
        "\n",
        "X = data['Text'].values\n",
        "Y = data['Label'].values.astype(int)\n",
        "\n",
        "X = [x.lower() for x in X]\n",
        "\n",
        "emoji_dict = {\n",
        "    0: \":red_heart:\",\n",
        "    1: \":baseball:\",\n",
        "    2: \":grinning_face_with_big_eyes:\",\n",
        "    3: \":disappointed_face:\",\n",
        "    4: \":fork_and_knife_with_plate:\"\n",
        "}\n",
        "\n",
        "def label_to_emoji(label):\n",
        "  return emoji.emojize(emoji_dict[label])\n",
        "\n",
        "\n",
        "#Embeddings\n",
        "#file = open('/content/sample_data/glove.6B.100d.txt','r', encoding = 'utf8')\n",
        "#content = file.readlines()\n",
        "#file.close()\n",
        "\n",
        "# content\n",
        "\n",
        "embeddings = {}\n",
        "with open('/content/sample_data/glove.6B.100d.txt','r', encoding = 'utf8') as file:\n",
        "  for line in file:\n",
        "    split_line = line.split()\n",
        "    word = split_line[0]\n",
        "    vector = np.array(split_line[1:], dtype = float)\n",
        "    embeddings[word] = vector\n",
        "\n",
        "  #def get_maxlen(data):\n",
        "    #maxlen = 0\n",
        "    #for sent in data:\n",
        "        #maxlen = max(maxlen, len(sent))\n",
        "    #return max(len(sent) for sent in data)\n",
        "\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(X)\n",
        "  word2index = tokenizer.word_index\n",
        "  Xtokens = tokenizer.texts_to_sequences(X)\n",
        "\n",
        "  maxlen = max(len(sent) for sent in Xtokens)\n",
        "  Xtrain = pad_sequences(Xtokens, maxlen = maxlen,  padding = 'post', truncating = 'post')\n",
        "  Ytrain = to_categorical(Y)\n",
        "\n",
        "  #check dataset\n",
        "  print(\"Label distribution:\\n\", data['Label'].value_counts())\n",
        "\n",
        "\n",
        "\n",
        "  #Model\n",
        "  embed_size = 100\n",
        "  embedding_matrix = np.zeros((len(word2index)+1, embed_size))\n",
        "\n",
        "  for word, i in word2index.items():\n",
        "      embed_vector = embeddings.get(word)\n",
        "      if embed_vector is not None:\n",
        "          embedding_matrix[i] = embed_vector\n",
        "\n",
        "\n",
        "  model = Sequential([\n",
        "    Embedding(input_dim = len(word2index) + 1,\n",
        "              output_dim = embed_size,\n",
        "              input_length = maxlen,\n",
        "              weights = [embedding_matrix],\n",
        "              trainable = False),\n",
        "    LSTM(64, return_sequences=True),\n",
        "    Dropout(0.5),\n",
        "    LSTM(32),\n",
        "    Dropout(0.5),\n",
        "    Dense(5, activation='softmax')\n",
        "])\n",
        "\n",
        "  model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "  model.summary()\n",
        "\n",
        "  #Train\n",
        "  model.fit(Xtrain, Ytrain, epochs = 30, batch_size = 32, validation_split=0.2)\n",
        "\n",
        "  model.save('emoji_model.h5')\n",
        "\n",
        "  import pickle\n",
        "  with open('tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "\n",
        "\n",
        "  #Test\n",
        "  test_sentences = [\"I feel good\", \"I feel upset\", \"let's eat dinner\"]\n",
        "  test_sentences = [s.lower() for s in test_sentences]\n",
        "\n",
        "\n",
        "  test_tokens = tokenizer.texts_to_sequences(test_sentences)\n",
        "  Xtest = pad_sequences(test_tokens, maxlen=maxlen, padding='post', truncating='post')\n",
        "\n",
        "  predictions = model.predict(Xtest)\n",
        "  predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "  # Output predictions\n",
        "  for i, sentence in enumerate(test_sentences):\n",
        "      print(f\"Input: {sentence} | Predicted Emoji: {label_to_emoji(predicted_labels[i])}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7O0t9mOK7FBF",
        "outputId": "85496774-101f-483c-d86e-0fcdc4710f28"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/590.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.1\n"
          ]
        }
      ]
    }
  ]
}